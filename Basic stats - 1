{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shiva191102/Excelr-Data-Sceince-Projects/blob/main/Basic%20stats%20-%201\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports & load data"
      ],
      "metadata": {
        "id": "sKVraHx_LGRc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "lIYdn1woOS1n",
        "outputId": "a8580603-bdd0-4e26-ab89-60cd4ac0b181"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'EastWestAirlines.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2257330808.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EastWestAirlines.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'EastWestAirlines.xlsx'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_excel(\"EastWestAirlines.xlsx\")\n",
        "print(\"Loaded shape:\", df.shape)\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine text columns into one document per row"
      ],
      "metadata": {
        "id": "bogjoooeL5H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine text columns\n",
        "df_text = df.astype(str).fillna('')"
      ],
      "metadata": {
        "id": "Xo-DzEx1L_1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join all columns into one textual document per row\n",
        "df_text['text'] = df_text.apply(\n",
        "    lambda row: \" \".join([str(x).strip() for x in row.values if str(x).strip() not in (\"nan\",\"None\",\"\")]),\n",
        "    axis=1)"
      ],
      "metadata": {
        "id": "fu7ZnKlTMJK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove empty rows\n",
        "df_text = df_text[df_text['text'].str.strip() != \"\"].reset_index(drop=True)\n",
        "print(\"Text rows:\", len(df_text))\n",
        "display(df_text[['text']].head(12))\n"
      ],
      "metadata": {
        "id": "sKOJZplwHoGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF vectorization + dimensionality reduction (TruncatedSVD)"
      ],
      "metadata": {
        "id": "Qyj6m9pEMfit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF + TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "texts = df_text['text'].tolist()\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X_tfidf = vectorizer.fit_transform(texts)\n",
        "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n",
        "\n",
        "# choose SVD components: at most 10 or n_samples-1\n",
        "n_components = min(10, max(2, X_tfidf.shape[0] - 1))\n",
        "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "X_reduced = svd.fit_transform(X_tfidf)\n",
        "print(\"SVD reduced shape:\", X_reduced.shape)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_reduced)\n"
      ],
      "metadata": {
        "id": "wUCRZcoQHz_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means: elbow & silhouette, choose best K, fit final model"
      ],
      "metadata": {
        "id": "sAARfA_SMslF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "max_k = min(6, X_scaled.shape[0] - 1)   # small dataset: limit K\n",
        "Ks = list(range(2, max_k + 1)) if max_k >= 2 else [2]\n",
        "\n",
        "inertias, sil_scores = [], []\n",
        "for k in Ks:\n",
        "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labs = km.fit_predict(X_scaled)\n",
        "    inertias.append(km.inertia_)\n",
        "    sil_scores.append(silhouette_score(X_scaled, labs))\n",
        "\n",
        "# plots (optional in notebook)\n",
        "plt.figure(figsize=(6,3)); plt.plot(Ks, inertias, marker='o'); plt.title(\"Elbow - inertia\"); plt.xlabel(\"K\"); plt.show()\n",
        "plt.figure(figsize=(6,3)); plt.plot(Ks, sil_scores, marker='o'); plt.title(\"Silhouette vs K\"); plt.xlabel(\"K\"); plt.show()\n",
        "\n",
        "best_k = Ks[int(np.argmax(sil_scores))]\n",
        "print(\"Ks tried:\", Ks)\n",
        "print(\"Silhouettes:\", [round(s,4) for s in sil_scores])\n",
        "print(\"Best K:\", best_k)\n",
        "\n",
        "kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10).fit(X_scaled)\n",
        "df_text['kmeans_cluster'] = kmeans.labels_\n",
        "print(\"KMeans counts:\\n\", df_text['kmeans_cluster'].value_counts().sort_index())\n"
      ],
      "metadata": {
        "id": "nHOhIvfvH3sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agglomerative (hierarchical) clustering for comparison"
      ],
      "metadata": {
        "id": "IhoBl5arM5uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=best_k)\n",
        "df_text['agg_cluster'] = agg.fit_predict(X_scaled)\n",
        "print(\"Agglomerative counts:\\n\", df_text['agg_cluster'].value_counts().sort_index())\n",
        "\n",
        "# silhouette for comparison\n",
        "print(\"Agglomerative silhouette:\", round(silhouette_score(X_scaled, df_text['agg_cluster']), 4))\n"
      ],
      "metadata": {
        "id": "A_bOvfkaH70M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DBSCAN grid search — find clusters and noise"
      ],
      "metadata": {
        "id": "ACYkUplRNEFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "results = []\n",
        "best_db_score = -1\n",
        "best_db_labels = None\n",
        "best_params = None\n",
        "\n",
        "eps_values = [0.3, 0.5, 0.7, 1.0]\n",
        "min_samples_list = [2, 3, 4, 5]\n",
        "\n",
        "for eps in eps_values:\n",
        "    for ms in min_samples_list:\n",
        "        db = DBSCAN(eps=eps, min_samples=ms)\n",
        "        labs = db.fit_predict(X_scaled)\n",
        "        n_clusters = len(set(labs)) - (1 if -1 in labs else 0)\n",
        "        if n_clusters <= 1:\n",
        "            score = -1\n",
        "        else:\n",
        "            try:\n",
        "                score = silhouette_score(X_scaled, labs)\n",
        "            except Exception:\n",
        "                score = -1\n",
        "        results.append((eps, ms, n_clusters, score))\n",
        "        if score > best_db_score:\n",
        "            best_db_score = score\n",
        "            best_db_labels = labs.copy()\n",
        "            best_params = (eps, ms)\n",
        "\n",
        "res_df = pd.DataFrame(results, columns=['eps','min_samples','n_clusters','silhouette']).sort_values(by='silhouette', ascending=False)\n",
        "display(res_df.head(8))\n",
        "print(\"Best DBSCAN:\", best_params, \"silhouette:\", best_db_score)\n",
        "\n",
        "if best_db_labels is not None:\n",
        "    df_text['dbscan_cluster'] = best_db_labels\n",
        "    print(\"DBSCAN counts (incl -1 noise):\\n\", df_text['dbscan_cluster'].value_counts().sort_index())\n"
      ],
      "metadata": {
        "id": "nyfaphpaH_fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto-generate short interpretation labels for each KMeans cluster"
      ],
      "metadata": {
        "id": "OWcZUngJNN2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create short labels for KMeans clusters using top TF-IDF words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# re-fit TF-IDF to get feature names (same config)\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X_tfidf = vectorizer.fit_transform(texts)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "cluster_labels = {}\n",
        "for c in sorted(df_text['kmeans_cluster'].unique()):\n",
        "    idx = df_text[df_text['kmeans_cluster'] == c].index.tolist()\n",
        "    if len(idx) == 0:\n",
        "        cluster_labels[c] = \"empty\"\n",
        "        continue\n",
        "    # sum tf-idf vectors of cluster rows -> top terms\n",
        "    cluster_vec = X_tfidf[idx].sum(axis=0).A1\n",
        "    top_indices = cluster_vec.argsort()[-5:][::-1]\n",
        "    top_words = [feature_names[i] for i in top_indices]\n",
        "    label = \", \".join(top_words[:5])\n",
        "    cluster_labels[c] = label\n",
        "\n",
        "# Map labels\n",
        "df_text['kmeans_label'] = df_text['kmeans_cluster'].map(cluster_labels)\n",
        "print(\"Cluster labels (kmeans):\")\n",
        "for k,v in cluster_labels.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "ZfT31CF9IOpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick visualization (PCA 2D) — optional but helpful"
      ],
      "metadata": {
        "id": "CBnfNBM5NV3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA 2D visualization for inspection\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], c=df_text['kmeans_cluster'], s=40)\n",
        "plt.title(\"KMeans clusters (PCA 2D)\")\n",
        "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\"); plt.show()\n",
        "\n",
        "if 'dbscan_cluster' in df_text.columns:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.scatter(X_pca[:,0], X_pca[:,1], c=df_text['dbscan_cluster'], s=40)\n",
        "    plt.title(\"DBSCAN clusters (PCA 2D)\")\n",
        "    plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\"); plt.show()\n"
      ],
      "metadata": {
        "id": "wO1Bzg1_IRwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the saved summary\n",
        "print(df_text[['kmeans_cluster','kmeans_label','agg_cluster']].value_counts(dropna=False).head(10))\n"
      ],
      "metadata": {
        "id": "BRmIXPYFIi_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e6jy2t3aKOws"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}